{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "359697d5",
   "metadata": {},
   "source": [
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 1. Introduction to LangChain\n",
    "*in Python*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d788b0",
   "metadata": {},
   "source": [
    "### **A. Introduction**\n",
    "*This introduction is based off Greg Kamradt's cookbook, which in turn is based off the [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "**Goal:** Provide an updated, introductory understanding of the components and use cases of LangChain. LangChain is, in essence, an AI framework. It has pros & cons which need to be understood.\n",
    "\n",
    "**Links:**\n",
    "* [LC Conceptual Documentation](https://docs.langchain.com/docs/)\n",
    "* [LC Python Documentation](https://python.langchain.com/en/latest/)\n",
    "* [LC Javascript/Typescript Documentation](https://js.langchain.com/docs/)\n",
    "* [LC Discord](https://discord.gg/6adMQxSpJS)\n",
    "* [www.langchain.com](https://langchain.com/)\n",
    "* [LC Twitter](https://twitter.com/LangChainAI)\n",
    "* Check out [ELI5](https://www.dictionary.com/e/slang/eli5/#:~:text=ELI5%20is%20short%20for%20%E2%80%9CExplain,a%20complicated%20question%20or%20problem.) examples and code snippets\n",
    "* For use cases check out [part 2](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb)\n",
    "* See [video tutorial](https://www.youtube.com/watch?v=2xxziIWmaSA) of this notebook\n",
    "\n",
    "### **B. What is LangChain?**\n",
    "> LangChain is a framework for developing applications powered by language models.\n",
    "\n",
    "LangChain makes the complicated parts of working & building with AI models easier. It helps do this in two ways:\n",
    "\n",
    "1. **Integration** - Bring external data, such as your files, other applications, and api data, to your LLMs\n",
    "2. **Agency** - Allow your LLMs to interact with it's environment via decision making. Use LLMs to help decide which action to take next\n",
    "\n",
    "### **C. Pros of LangChain**\n",
    "1. **Components** - LangChain makes it easy to swap out abstractions and components necessary to work with language models. Though LLMs can be straightforward (text-in, text-out) you'll quickly run into friction points that LangChain helps with once you develop more complicated applications.\n",
    "\n",
    "2. **Customized Chains** - LangChain provides out of the box support for using and customizing 'chains' - a series of actions strung together.\n",
    "\n",
    "3. **Speed üö¢** - This team ships insanely fast. You'll be up to date with the latest LLM features.\n",
    "\n",
    "4. **Open source** - The components are easily modifiable.\n",
    "\n",
    "5. **Community üë•** - Wonderful discord and community support, meet ups, hackathons, etc.\n",
    "\n",
    "### **D. Cons of LangChain**\n",
    "1. **Right tool for the right job** - LangChain, Kubernetes, CrewAI are tools which are useful to automate workflows when the workflow *is not well defined.* In most cases they are, in which case these tools may hinder productivity and efficiency. \n",
    "\n",
    "2. **Inefficient Token Usage** - One of the significant concerns raised about Langchain is its token counting function, which can be inefficient for small datasets. Use `Tiktoken` instead.\n",
    "\n",
    "3. **Terrible Documentation.**\n",
    "\n",
    "4. **Too Much Obfuscation, Overly Abundant ‚ÄòHelper‚Äô Functions** - Too much boiler plate functions which can introduce bugs and ineffecient code into an already process intensive application.\n",
    "\n",
    "5. **Inconsistent Behavior and Hidden Details** - LangChain has been criticized for hiding important details and having inconsistent behavior, which can lead to unexpected issues in production systems. For example, developers have observed an intriguing aspect of the Langchain ConversationRetrievalChain, which involves the rephrasing of input questions. This rephrasing can sometimes be so extensive that it disrupts the natural flow of the conversation and takes it out of context.\n",
    "\n",
    "6. **Lack of a Standard Interoperable Datatype** - Another drawback of Langchain is its absence of a standard way to represent data. This lack of uniformity can hinder integration with other frameworks and tools, making it challenging to work within a broader ecosystem of machine learning tools.\n",
    "\n",
    "### **E. Summary**\n",
    "LangChain has both pros and cons, however the main reason to use it would be as a base for individual projects and customize its components to fit ones needs. \n",
    "\n",
    "*Note: This introduction will not cover all aspects of LangChain. It's contents have been curated to get you to building & impact as quick as possible. For more, please check out [LangChain Conceptual Documentation](https://docs.langchain.com/docs/)*\n",
    "\n",
    "You'll need API keys for the various models that are included in this tutorial. You can have it as an environement variable, in an .env file where this jupyter notebook lives, or insert it below where 'YourAPIKey' is. Have if you have questions on this, put these instructions into [ChatGPT](https://chat.openai.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9815081",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# openai_api_key=os.getenv('OPENAI_API_KEY', 'YourAPIKey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb564d",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 2. LangChain Components\n",
    "*Schema - Nuts and Bolts of working with Large Language Models (LLMs)*\n",
    "\n",
    "----\n",
    "### **A. Text**\n",
    "The natural language way to interact with LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e0dc06c",
   "metadata": {
    "hide_input": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What day comes after Friday?'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You'll be working with simple strings (that'll soon grow in complexity!)\n",
    "my_text = \"What day comes after Friday?\"\n",
    "my_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941564b6-93fc-41f4-a25b-0baaa9a26e47",
   "metadata": {},
   "source": [
    "You can *invoke* a model directly using text as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67cce03d-81d0-44dc-a3ec-c4850d93a9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full result:\n",
      "content='Saturday\\n' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []} id='run-d9bdff30-ec90-4d13-923c-abecfe22256a-0' usage_metadata={'input_tokens': 7, 'output_tokens': 2, 'total_tokens': 9, 'input_token_details': {'cache_read': 0}} \n",
      "\n",
      "Content only:\n",
      "Saturday\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# Invoke the model with a message\n",
    "result = model.invoke(my_text)\n",
    "print(\"Full result:\")\n",
    "print(result, \"\\n\")\n",
    "print(\"Content only:\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39eb39",
   "metadata": {},
   "source": [
    "### **B. Chat Messages**\n",
    "Like text, but specified with a message type (System, Human, AI)\n",
    "\n",
    "* **System** - Helpful background context that tell the AI what to do\n",
    "* **Human** - Messages that are intented to represent the user\n",
    "* **AI** - Messages that show what the AI responded with (this can also be used to *teach* the AI what to respond to)\n",
    "\n",
    "For more, see OpenAI's [documentation](https://platform.openai.com/docs/guides/chat/introduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b0935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# This it the language model we'll use. We'll talk about what we're doing below in the next section.\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d2f7af",
   "metadata": {},
   "source": [
    "Now let's create a few messages that simulate a chat experience with a bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "878d6a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ENTIRE MESSAGE:\n",
      "content='How about a fresh caprese salad with tomatoes, mozzarella, basil, and a drizzle of balsamic glaze?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 39, 'total_tokens': 62, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_d02d531b47', 'finish_reason': 'stop', 'logprobs': None} id='run-fddb4489-01ad-42b1-b551-51b8a40a0607-0' usage_metadata={'input_tokens': 39, 'output_tokens': 23, 'total_tokens': 62, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "\n",
      "THE CONTENT ALONE:\n",
      "How about a fresh caprese salad with tomatoes, mozzarella, basil, and a drizzle of balsamic glaze?\n"
     ]
    }
   ],
   "source": [
    "# SystemMessage:\n",
    "#   Message for priming AI behavior, usually passed in as the first of a sequenc of input messages.\n",
    "# HumanMessagse:\n",
    "#   Message from a human to the AI model.\n",
    "\n",
    "messages = [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out what to eat in one short sentence\"),\n",
    "        HumanMessage(content=\"I like tomatoes, what should I eat?\")\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"THE ENTIRE MESSAGE:\\n{result}\\n\")\n",
    "print(f\"THE CONTENT ALONE:\\n{result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a425aaa",
   "metadata": {},
   "source": [
    "<br/>\n",
    "You can also pass more chat history with responses from the AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd3fe88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'While in Nice, take a stroll along the Promenade des Anglais, explore the old town (Vieux Nice), and visit the Marc Chagall National Museum.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "        SystemMessage(content=\"You are a nice AI bot that helps a user figure out where to travel in one short sentence\"),\n",
    "        HumanMessage(content=\"I like the beaches where should I go?\"),\n",
    "        AIMessage(content=\"You should go to Nice, France\"),\n",
    "        HumanMessage(content=\"What else should I do when I'm there?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages).content # Note that the model has inferred where I was from the chat log that was provided"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5ee37a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "You can also exclude the system message if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238a49f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The day that comes after Thursday is Friday.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "        HumanMessage(content=\"What day comes after Thursday?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bf9634",
   "metadata": {},
   "source": [
    "### **C. Documents**\n",
    "An object that holds a piece of text and metadata (more information about that text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bbf58b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ad9bef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'my_document_id': 234234, 'my_document_source': 'The LangChain Papers', 'my_document_create_time': 1680013019}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\",\n",
    "         metadata={\n",
    "             'my_document_id' : 234234,\n",
    "             'my_document_source' : \"The LangChain Papers\",\n",
    "             'my_document_create_time' : 1680013019\n",
    "         })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd19754",
   "metadata": {},
   "source": [
    "But you don't have to include metadata if you don't want to. However, the metadata helps searching documents in a library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0798d3ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"This is my document. It is full of text that I've gathered from other places\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Document(page_content=\"This is my document. It is full of text that I've gathered from other places\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e462b5d",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 3. Models - The interface to the AI brains\n",
    "*Chat model alternatives*\n",
    "\n",
    "----\n",
    "LLMs come in many different flavors, here are a few. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27fe982",
   "metadata": {},
   "source": [
    "###  **A. Language Models**\n",
    "A model that does text in ‚û°Ô∏è text out!\n",
    "\n",
    "*Various models are available for use. See more models [here](https://platform.openai.com/docs/models)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b1a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Setup environment variables and messages\n",
    "load_dotenv()\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Solve the following math problems\"),\n",
    "    HumanMessage(content=\"What is 81 divided by 9?\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ab62e-a94e-419c-a540-dc0a36269271",
   "metadata": {},
   "source": [
    "Below is the LangChain OpenAI Chat Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3845efee-6665-4ea0-98ce-1a91ec634848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from OpenAI: 81 divided by 9 is 9.\n"
     ]
    }
   ],
   "source": [
    "# ---- LangChain OpenAI Chat Model Example ---- #\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Invoke the model with messages\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from OpenAI: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c727cf-3216-40d6-8071-b563005a68cd",
   "metadata": {},
   "source": [
    "Below is the Google Chat Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5aafa89-4b8b-4893-8db2-61fa7f232e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer from Google: 81 divided by 9 is 9.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Google Chat Model Example ---- #\n",
    "\n",
    "# https://console.cloud.google.com/gen-app-builder/engines\n",
    "# https://ai.google.dev/gemini-api/docs/models/gemini\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from Google: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b383faa2-e9a8-4447-ac64-836f3a87d119",
   "metadata": {},
   "source": [
    "Below is the Anthropic Chat Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38586edc-7e67-43c1-996a-5882f3a1daa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Anthropic Chat Model Example ---- #\n",
    "\n",
    "# Create a Anthropic model\n",
    "# Anthropic models: https://docs.anthropic.com/en/docs/models-overview\n",
    "model = ChatAnthropic(model=\"claude-3-opus-20240229\")\n",
    "\n",
    "result = model.invoke(messages)\n",
    "print(f\"Answer from Anthropic: {result.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef89bfa",
   "metadata": {},
   "source": [
    "### **B. Chat Models**\n",
    "A model that takes a series of messages and returns a message output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf091777",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "# This it the language model we'll use. Note that the temperature has been set to 1, hence the model's responses become more stochastic. \n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4260711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why don‚Äôt you just take a bus? It‚Äôs either that or just wait for your dreams to take off‚Äîeither way, you need to pack!'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "        SystemMessage(content=\"You are an unhelpful AI bot that makes a joke at whatever the user says\"),\n",
    "        HumanMessage(content=\"I would like to go to New York, how should I do this?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c028f9",
   "metadata": {},
   "source": [
    "### **C. Function Calling Models**\n",
    "*Deprecated. Doesn't work at present!*\n",
    "\n",
    "[Function calling models](https://openai.com/blog/function-calling-and-other-api-updates) are similar to Chat Models but with a little extra flavor. They are fine tuned to give structured data outputs.\n",
    "\n",
    "This comes in handy when you're making an API call to an external service or doing extraction. When you use function calling, the model never actually executes functions itself - instead, it simply generates parameters that can be used to call your function. You are then responsible for handling how the function is executed in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1020ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)\n",
    "\n",
    "messages = [\n",
    "         SystemMessage(content=\"You are an helpful AI bot\"),\n",
    "         HumanMessage(content=\"What‚Äôs the weather like in Sydney right now?\")\n",
    "]\n",
    "\n",
    "functions = [\n",
    "  {\n",
    "      \"name\": \"get_current_weather\",\n",
    "      \"description\": \"Get the current weather in a given location\",\n",
    "      \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "              \"type\": \"object\",\n",
    "              \"properties\": {\n",
    "                  \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n",
    "                  \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                  \"required\": [\"location\"]\n",
    "              },\n",
    "          },\n",
    "      },\n",
    "  }\n",
    "]\n",
    "\n",
    "complete_message_list = [messages, functions]\n",
    "\n",
    "#model.invoke(complete_message_list) # Doesn't work!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f399a1d",
   "metadata": {},
   "source": [
    "See the extra `additional_kwargs` that is passed back to us? We can take that and pass it to an external API to get data. It saves the hassle of doing output parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b70f23",
   "metadata": {},
   "source": [
    "### **D. Text Embedding Model**\n",
    "Change your text into a vector (a series of numbers that hold the semantic 'meaning' of your text). Mainly used when comparing two pieces of text together.\n",
    "\n",
    "*BTW: Semantic means 'relating to meaning in language or logic.'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1655de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "text = \"Hi! It's time for the beach\"\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddc5a368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a sample: [-0.00022214034106582403, -0.0031126115936785936, -0.0010768607025966048, -0.019214099273085594, -0.015184946358203888]...\n",
      "Your embedding is length 1536\n"
     ]
    }
   ],
   "source": [
    "text_embedding = embeddings.embed_query(text)\n",
    "print (f\"Here's a sample: {text_embedding[:5]}...\")\n",
    "print (f\"Your embedding is length {len(text_embedding)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38fe99f",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 4. Prompts - Text generally used as instructions to your model\n",
    "*Prompts and templates - basics of LangChain*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9318ed",
   "metadata": {},
   "source": [
    "### **A. Prompt**\n",
    "What you'll pass to the underlying model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d270239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The problem is that tomorrow after Monday should be Tuesday, not Wednesday.  The statement skips a day.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt Template Docs:\n",
    "#   https://python.langchain.com/v0.2/docs/concepts/#prompt-templateshttps://python.langchain.com/v0.2/docs/concepts/#prompt-templates\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# I like to use three double quotation marks for my prompts because it's easier to read\n",
    "prompt = \"\"\"\n",
    "Today is Monday, tomorrow is Wednesday.\n",
    "\n",
    "What is wrong with that statement?\n",
    "\"\"\"\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "model.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74988254",
   "metadata": {},
   "source": [
    "### **B. Prompt Templates**\n",
    "An object that helps create prompts based on a combination of user input, other non-static information and a fixed template string.\n",
    "\n",
    "Think of it as an [f-string](https://realpython.com/python-f-strings/) in python but for prompts.\n",
    "\n",
    "*Advanced: Check out LangSmithHub(https://smith.langchain.com/hub) for many more communit prompt templates.*\n",
    "\n",
    "In reality this is fairly useless by itself, as you could simply use `f-strings` or `\"string\".replace()` statements in Python, which would be less error prone as well - that is, unless you are attempting to dynamically generating examples for the LLM to be used in prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9cccdf17-3f5b-4e6f-b806-29b097d7a37c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "I really want to travel to Rome. What should I do there?\n",
      "\n",
      "Respond in one short sentence.\n",
      "\n",
      "-----------\n",
      "LLM Output: Visit iconic sites like the Colosseum, Vatican City, and the Trevi Fountain while enjoying authentic Italian cuisine.\n"
     ]
    }
   ],
   "source": [
    "# PART 1: Template basics\n",
    "# Notice \"location\" below, that is a placeholder for another value later\n",
    "template = \"\"\"\n",
    "I really want to travel to {location}. What should I do there? \n",
    "\n",
    "Respond in one short sentence.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"location\"], template=template)\n",
    "final_prompt = prompt.format(location='Rome')\n",
    "\n",
    "print (f\"Final Prompt: {final_prompt}\")\n",
    "print (\"-----------\")\n",
    "print (f\"LLM Output: {model.invoke(final_prompt).content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d20e7-d80f-401c-b67e-a99dbd6f80ef",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "`ChatPromptTemplate` are a more recent addition to the prompt templates, but seem only to add to the clutter without any meaningful value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abcc212d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----Prompt from Template-----\n",
      "\n",
      "messages=[HumanMessage(content='Tell me a joke about cats.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# PART 2: Create a ChatPromptTemplate using a template string\n",
    "template = \"Tell me a joke about {topic}.\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(\"\\n-----Prompt from Template-----\\n\")\n",
    "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "305ceff1-13c0-40c5-8e48-98c66084377e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with Multiple Placeholders -----\n",
      "\n",
      "messages=[HumanMessage(content='You are a helpful assistant.\\nHuman: Tell me a funny story about a panda.\\nAssistant:', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# PART 3: Prompt with Multiple Placeholders\n",
    "template_multiple = \"\"\"You are a helpful assistant.\n",
    "Human: Tell me a {adjective} story about a {animal}.\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt_multiple = ChatPromptTemplate.from_template(template_multiple)\n",
    "prompt = prompt_multiple.invoke({\"adjective\": \"funny\", \"animal\": \"panda\"})\n",
    "\n",
    "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2acd25ca-7761-4fc9-946f-2e8f960b008d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# PART 4: Prompt with System and Human Messages (Using Tuples)\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9ceab9-1d69-422d-9468-587657c93907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me 3 jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Extra information about Part 3. This is what WORKS:\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    HumanMessage(content=\"Tell me 3 jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\"})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516a24c4-3d40-4b1a-ae97-aee8cc163c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- Prompt with System and Human Messages (Tuple) -----\n",
      "\n",
      "messages=[SystemMessage(content='You are a comedian who tells jokes about lawyers.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me {joke_count} jokes.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Extra information about Part 3. This does NOT work, unless tuples are used:\n",
    "messages = [\n",
    "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
    "    HumanMessage(content=\"Tell me {joke_count} jokes.\"),\n",
    "]\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt = prompt_template.invoke({\"topic\": \"lawyers\", \"joke_count\": 3})\n",
    "\n",
    "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed40bac2",
   "metadata": {},
   "source": [
    "### **C. Example Selectors**\n",
    "An easy way to select from a series of examples that allow you to dynamic place in-context information into your prompt. Often used when your task is nuanced or you have a large list of examples.\n",
    "\n",
    "Check out different types of example selectors [here](https://python.langchain.com/docs/modules/model_io/prompts/example_selectors/).\n",
    "\n",
    "If you want an overview on why examples are important (prompt engineering), check out [this video](https://www.youtube.com/watch?v=dOxUroR57xs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aaf36cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_core.prompts import FewShotPromptTemplate, PromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_prompt = PromptTemplate(input_variables=[\"input\", \"output\"], template=\"Input: {input}\\nOutput: {output}\")\n",
    "\n",
    "# Examples of locations that nouns are found\n",
    "examples = [\n",
    "    {\"input\": \"pirate\", \"output\": \"ship\"},\n",
    "    {\"input\": \"pilot\", \"output\": \"plane\"},\n",
    "    {\"input\": \"driver\", \"output\": \"car\"},\n",
    "    {\"input\": \"tree\", \"output\": \"ground\"},\n",
    "    {\"input\": \"bird\", \"output\": \"nest\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12b4798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SemanticSimilarityExampleSelector will select examples that are similar to your input by semantic meaning\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    # The list of examples available to select from.\n",
    "    examples,\n",
    "    # The embedding class used to produce embeddings which are used to measure semantic similarity.\n",
    "    OpenAIEmbeddings(),\n",
    "    # The VectorStore class that is used to store the embeddings and do a similarity search over.\n",
    "    FAISS,\n",
    "    # The number of examples to produce.\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cf30107",
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_prompt = FewShotPromptTemplate(\n",
    "    # The object that will help select examples\n",
    "    example_selector=example_selector,\n",
    "    # Your prompt\n",
    "    example_prompt=example_prompt,\n",
    "    # Customizations that will be added to the top and bottom of your prompt\n",
    "    prefix=\"Give the location an item is usually found in\",\n",
    "    suffix=\"Input: {noun}\\nOutput:\",\n",
    "    # What inputs your prompt will receive\n",
    "    input_variables=[\"noun\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "369442bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECTING PEOPLE BY PROFESSION\n",
      "Give the location an item is usually found in\n",
      "\n",
      "Input: driver\n",
      "Output: car\n",
      "\n",
      "Input: pilot\n",
      "Output: plane\n",
      "\n",
      "Input: student\n",
      "Output:\n",
      "\n",
      "SELECTING NOUNS BY THINGS\n",
      "Give the location an item is usually found in\n",
      "\n",
      "Input: tree\n",
      "Output: ground\n",
      "\n",
      "Input: bird\n",
      "Output: nest\n",
      "\n",
      "Input: flower\n",
      "Output:\n"
     ]
    }
   ],
   "source": [
    "print(\"SELECTING PEOPLE BY PROFESSION\")\n",
    "print(similar_prompt.format(noun=\"student\")) # Note that this finds the appropriate example similar to the noun\n",
    "\n",
    "print(\"\\nSELECTING NOUNS BY THINGS\")\n",
    "print(similar_prompt.format(noun=\"flower\")) # Note that this finds the appropriate example similar to the noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bb910f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'garden'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(similar_prompt.format(noun=\"flower\")).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8474c91d",
   "metadata": {},
   "source": [
    "### **D. Output Parsers Method 1: Prompt Instructions & String Parsing**\n",
    "A helpful way to format the output of a model. Usually used for structured output. LangChain has a bunch more output parsers listed on their [documentation](https://python.langchain.com/docs/modules/model_io/output_parsers).\n",
    "\n",
    "Two big concepts:\n",
    "\n",
    "**1. Format Instructions** - A autogenerated prompt that tells the LLM how to format it's response based off your desired result\n",
    "\n",
    "**2. Parser** - A method which will extract your model's text output into a desired structure (usually json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58353756",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from dotenv import load_dotenv \n",
    "\n",
    "# Setup environment variables and messages\n",
    "load_dotenv()\n",
    "\n",
    "# Create a ChatOpenAI model\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa59be3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i. How you would like your RESPONSE structured\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"bad_string\", description=\"This a poorly formatted user input string\"),\n",
    "    ResponseSchema(name=\"good_string\", description=\"This is your response, a reformatted response\")\n",
    "]\n",
    "\n",
    "# ii. How you would like to PARSE your RESPONSE\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1079f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# See the format_instructions you created for formatting\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "print (format_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aaae5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You will be given a poorly formatted string from a user.\n",
      "Reformat it and make sure all the words are spelled correctly.\n",
      "\n",
      "FORMAT INSTRUCTIONS:\n",
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"bad_string\": string  // This a poorly formatted user input string\n",
      "\t\"good_string\": string  // This is your response, a reformatted response\n",
      "}\n",
      "```\n",
      "\n",
      "USER INPUT:\n",
      "welcom to califonya!\n",
      "\n",
      "YOUR RESPONSE:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iii. This is the template that puts it all together\n",
    "template = \"\"\"\n",
    "You will be given a poorly formatted string from a user.\n",
    "Reformat it and make sure all the words are spelled correctly.\n",
    "\n",
    "FORMAT INSTRUCTIONS:\n",
    "{format_instructions}\n",
    "\n",
    "USER INPUT:\n",
    "{user_input}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"user_input\"], partial_variables={\"format_instructions\": format_instructions}, template=template)\n",
    "promptValue = prompt.format(user_input=\"welcom to califonya!\")\n",
    "print(promptValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b116bb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"bad_string\": \"welcom to califonya!\",\n",
      "\t\"good_string\": \"Welcome to California!\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "llm_output = model.invoke(promptValue).content\n",
    "print(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "985aa814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bad_string': 'welcom to califonya!', 'good_string': 'Welcome to California!'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07045ae3",
   "metadata": {},
   "source": [
    "### **E. Output Parsers Method 2: OpenAI Fuctions**\n",
    "When OpenAI released function calling, the game changed. This is recommended method when starting out.\n",
    "\n",
    "They trained models specifically for outputing structured data. It became super easy to specify a Pydantic schema and get a structured output.\n",
    "\n",
    "There are many ways to define your schema, I prefer using Pydantic Models because of how organized they are. Feel free to reference OpenAI's [documention](https://platform.openai.com/docs/guides/gpt/function-calling) for other methods.\n",
    "\n",
    "In order to use this method you'll need to use a model that supports [function calling](https://openai.com/blog/function-calling-and-other-api-updates#:~:text=Developers%20can%20now%20describe%20functions%20to%20gpt%2D4%2D0613%20and%20gpt%2D3.5%2Dturbo%2D0613%2C). I'll use `gpt-4o-mini`\n",
    "\n",
    "<br/>**i. Example: Simple**\n",
    "<br/>Let's get started by defining a simple model for us to extract from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3593699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Identifying information about a person.\"\"\"\n",
    "\n",
    "    name: str = Field(..., description=\"The person's name\")\n",
    "    age: int = Field(..., description=\"The person's age\")\n",
    "    fav_food: Optional[str] = Field(None, description=\"The person's favorite food\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17033d15",
   "metadata": {},
   "source": [
    "Then let's create a chain (more on this later) that will do the extracting for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b7be09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='Sally' age=13 fav_food=None\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "chain = model.with_structured_output(Person)\n",
    "\n",
    "person = chain.invoke(\"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\")\n",
    "print(person)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37370210",
   "metadata": {},
   "source": [
    "Notice how we only have data on one person from that list? That is because we didn't specify we wanted multiple. Let's change our schema to specify that we want a list of people if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4ad5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "\n",
    "class People(BaseModel):\n",
    "    \"\"\"Identifying information about all people in a text.\"\"\"\n",
    "\n",
    "    people: Sequence[Person] = Field(..., description=\"The people in the text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2bc127",
   "metadata": {},
   "source": [
    "Now we'll call for People rather than Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ba430d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people=[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)]\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_structured_output(People)\n",
    "\n",
    "people = chain.invoke(\"Sally is 13, Joey just turned 12 and loves spinach. Caroline is 10 years older than Sally.\")\n",
    "print(people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db9b8b",
   "metadata": {},
   "source": [
    "Let's do some more parsing with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "808ec329-22c9-4383-ba6a-58797b84652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caroline\n",
      "23\n",
      "None \n",
      "\n",
      "[Person(name='Sally', age=13, fav_food=None), Person(name='Joey', age=12, fav_food='spinach'), Person(name='Caroline', age=23, fav_food=None)]\n",
      "Sally 13 None\n",
      "Joey 12 spinach\n",
      "Caroline 23 None\n"
     ]
    }
   ],
   "source": [
    "# From the Person class\n",
    "print(person.name)\n",
    "print(person.age)\n",
    "print(person.fav_food, \"\\n\")\n",
    "\n",
    "# From the People class\n",
    "print(people.people)\n",
    "for person in people.people: print(person.name, person.age, person.fav_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10def8-8787-4de3-9d76-ea0ab6438a87",
   "metadata": {},
   "source": [
    "<br/>**ii. Example: Enum**\n",
    "<br/>Now let's parse when a product from a list is mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6616a735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "class Product(str, enum.Enum):\n",
    "    CRM = \"CRM\"\n",
    "    VIDEO_EDITING = \"VIDEO_EDITING\"\n",
    "    HARDWARE = \"HARDWARE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5250ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Products(BaseModel):\n",
    "    \"\"\"Identifying products that were mentioned in a text\"\"\"\n",
    "    \n",
    "    products: Sequence[Product] = Field(..., description=\"The products mentioned in a text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd7e0bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "products=[<Product.HARDWARE: 'HARDWARE'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>]\n",
      "products=[<Product.CRM: 'CRM'>, <Product.VIDEO_EDITING: 'VIDEO_EDITING'>, <Product.HARDWARE: 'HARDWARE'>]\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_structured_output(Products)\n",
    "\n",
    "# Partial match\n",
    "print(chain.invoke(\"This computer is great. Love the hardware. The graphics card is cool too. Love the video editing on it.\"))\n",
    "\n",
    "# Full match\n",
    "print(chain.invoke(\"The CRM in this demo is great. Love the hardware. The microphone is also cool. Love the video editing\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b43cec2",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 5. Indexes - Structuring documents to LLMs can work with them\n",
    "*Memory storage - basics of LangChain*\n",
    "\n",
    "---- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f904e9",
   "metadata": {},
   "source": [
    "### **A. Document Loaders**\n",
    "Easy ways to import data from other sources. Shared functionality with [OpenAI Plugins](https://openai.com/blog/chatgpt-plugins) [specifically retrieval plugins](https://github.com/openai/chatgpt-retrieval-plugin).\n",
    "\n",
    "See a [big list](https://python.langchain.com/en/latest/modules/indexes/document_loaders.html) of document loaders here. A bunch more on [Llama Index](https://llamahub.ai/) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4719d4",
   "metadata": {},
   "source": [
    "<br/>**i. Online news websites: e.g. HackerNews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ba88e05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import HNLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee693520",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = HNLoader(\"https://news.ycombinator.com/item?id=34422627\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88d89ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e814f930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 76 comments\n",
      "Here's a sample:\n",
      "\n",
      "Ozzie_osman on Jan 18, 2023  \n",
      "             | next [‚Äì] \n",
      "\n",
      "LangChain is awesome. For people not sure what it's doing, large language models (LLMs) are veOzzie_osman on Jan 18, 2023  \n",
      "             | parent | next [‚Äì] \n",
      "\n",
      "Also, another library to check out is GPT Index (https://github.com/jerryjliu/gpt_ind\n"
     ]
    }
   ],
   "source": [
    "print (f\"Found {len(data)} comments\")\n",
    "print (f\"Here's a sample:\\n\\n{''.join([x.page_content[:150] for x in data[:2]])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564583f",
   "metadata": {},
   "source": [
    "<br/>**ii. Online data repos: e.g. Books from Gutenberg Project**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "72964fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import GutenbergLoader\n",
    "\n",
    "loader = GutenbergLoader(\"https://www.gutenberg.org/cache/epub/2148/pg2148.txt\")\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47140a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " just after dark one gusty evening in the autumn of 18-,\n",
      "\n",
      "\n",
      "      I was enjoying the twofold l\n"
     ]
    }
   ],
   "source": [
    "print(data[0].page_content[1890:1984])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1386b0",
   "metadata": {},
   "source": [
    "<br/>**iii. Websites: URLs and webpages**\n",
    "<br/>Let's try it out with [Paul Graham's website](http://www.paulgraham.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46a54e7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New: Writes and Write-Nots | Founder Mode Want to start a startup? Get funded by Y Combinator . ¬© mmxxv pg'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "\n",
    "urls = [\"http://www.paulgraham.com/\"]\n",
    "\n",
    "loader = UnstructuredURLLoader(urls=urls)\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "data[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3426a835-1371-4046-bacb-123e02368851",
   "metadata": {},
   "source": [
    "<br/>**iv. Text file: Text document loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "068e5a02-beb3-4e8b-b988-002f08652c38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TISSUES AND STRUCTURES\n",
      "1. The body is composed of four basic tissues - epithelium, connective tissue, muscle and nerve.\n",
      "2. Skin consists of two elements: epithelium/epidermis & appendages (ectodermal in origin) and connective tissue (mesodermal in origin).\n",
      "3. The epidermis is of the stratified squam\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader('./Data/Anatomy-General.txt')\n",
    "\n",
    "data = loader.load()\n",
    "\n",
    "print(data[0].page_content[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff33eca-7d0f-4899-b978-2017012f2634",
   "metadata": {},
   "source": [
    "<br/>**iv. Directory: Load documents from a directory**\n",
    "<br/>LangChain's DirectoryLoader implements functionality for reading files from disk into LangChain Document objects.\n",
    "\n",
    "`DirectoryLoader` accepts a `loader_cls` kwarg, which defaults to `UnstructuredLoader`. Unstructured supports parsing for a number of formats, such as PDF and HTML. Here we use it to read text files.\n",
    "\n",
    "We can use the `glob` parameter to control which files to load. Note that here it doesn't load the `.rst` file or the `.html` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89b83427-1adb-400c-8d4f-8950211b4f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files in directory: 25 \n",
      "\n",
      "MYOTOMES & MUSCLES-THIGH\n",
      "\n",
      "----------\n",
      "\n",
      "MYOTOMES & MUSCLES-LEG\n",
      "\n",
      "LEG MUSCLES-ANKLE\n",
      "\n",
      "1. Ankle dorsiflexion (L4 + 5) => Tibialis anterior; Extensor hallucis longus; Extensor digitorum longus.\n",
      "\n",
      "2. Ankle plantarflexion (S1 + S2) => Gastro-soleus complex; Tibialis posterior; Flexor hallucis longus; Flexor d\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader # You will need to install python-magic-bin for this\n",
    "\n",
    "# Load text files from the directory\n",
    "loader = DirectoryLoader(\"./Data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "print(\"Number of text files in directory:\", len(docs), \"\\n\")\n",
    "\n",
    "# Print out the partial contentx of a single text file\n",
    "print(docs[3].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7d28d-d0df-443b-bb5a-0ec3d92aa33b",
   "metadata": {},
   "source": [
    "By default this uses the `UnstructuredLoader` class. To customize the loader, specify the loader class in the `loader_cls` kwarg. Below we show an example using `TextLoader`. Note that while the `UnstructuredLoader` parses Markdown headers, `TextLoader` does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a17f4708-fb22-4b1c-bce2-24b22182fa4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text files in directory: 25 \n",
      "\n",
      "MYOTOMES & MUSCLES-THIGH\n",
      "\n",
      "----------\n",
      "\n",
      "MYOTOMES & MUSCLES-LEG\n",
      "\n",
      "LEG MUSCLES-ANKLE\n",
      "\n",
      "1. Ankle dorsiflexion (L4 + 5) => Tibialis anterior; Extensor hallucis longus; Extensor digitorum longus.\n",
      "\n",
      "2. Ankle plantarflexion (S1 + S2) => Gastro-soleus complex; Tibialis posterior; Flexor hallucis longus; Flexor d\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Load text files from the directory\n",
    "loader = DirectoryLoader(\"./Data\", glob=\"**/*.txt\")\n",
    "docs = loader.load()\n",
    "print(\"Number of text files in directory:\", len(docs), \"\\n\")\n",
    "\n",
    "# Print out the partial contentx of a single text file\n",
    "print(docs[3].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9601db",
   "metadata": {},
   "source": [
    "### **B. Text Splitters**\n",
    "Often times your document is too long (like a book) for your LLM. You need to split it up into chunks. Text splitters help with this.\n",
    "\n",
    "There are many ways you could split your text into chunks, experiment with [different ones](https://python.langchain.com/en/latest/modules/indexes/text_splitters.html) to see which is best for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95713e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 1 long document!\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# This is a long document we can split up.\n",
    "with open('./Data/Anatomy-General.txt') as f: pg_work = f.read()\n",
    "    \n",
    "print (f\"You have {len([pg_work])} long document!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d19acb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 150,\n",
    "    chunk_overlap  = 20,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([pg_work])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3090f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 287 documents\n"
     ]
    }
   ],
   "source": [
    "print (f\"You have {len(texts)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87a0f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview:\n",
      "TISSUES AND STRUCTURES\n",
      "1. The body is composed of four basic tissues - epithelium, connective tissue, muscle and nerve. \n",
      "\n",
      "2. Skin consists of two elements: epithelium/epidermis & appendages (ectodermal in origin) and connective tissue (mesodermal in origin).\n"
     ]
    }
   ],
   "source": [
    "print (\"Preview:\")\n",
    "print (texts[0].page_content, \"\\n\")\n",
    "print (texts[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9e670d",
   "metadata": {},
   "source": [
    "There are a ton of different ways to do text splitting and it really depends on your retrieval strategy and application design. Check out more splitters [here](https://python.langchain.com/docs/how_to/#text-splitters).\n",
    "\n",
    "Splitting by character is the simplest method. This splits based on a given character sequence, which defaults to `\"\\n\\n\"`. Chunk length is measured by number of characters.\n",
    "\n",
    "`CharacterTextSplitter` will only split on separator (which is `'\\n\\n'` by default). `chunk_size` is the maximum chunk size that will be split if splitting is possible. If a string starts with n characters, has a separator, and has `m` more characters before the next separator then the first chunk size will be `n` if `chunk_size < n + m + len(separator)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b50dcd1-d9da-4983-9efb-8af136f090a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='SKIN GLANDS\n",
      "1. Two types - sweat (eccrine/apocrine) & sebaceous glands.\n",
      "2. Sweat glands are distributed all over skin except on the margins of the lips, glans penis and tympanic membranes. Greatest concentration - palms, soles, face.\n",
      "3. There are two types - eccrine and apocrine. The majority are ecrine glands whose purpose is to deliver water to the body surface and so assist in temperature regulation.\n",
      "4. The apocrine glands are larger and confined to the axillae, areolae of the breasts and urogenital regions (breasts are modified apocrine glands).\n",
      "5. Sebaceous glands are confined to hairy skin where they open by short ducts into the side of a hair follicle. At eyelids, lips, papillae of breasts and labia minora they open directly onto the skin.'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# i. Load an example document\n",
    "with open(\"./Data/Anatomy-General.txt\") as f: data = f.read()\n",
    "\n",
    "# ii. Disable the text splitters verbose output\n",
    "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
    "\n",
    "# iii. Split the text - setting chunk size to a small number is perfect to retrieve individual cards from an Anki deck\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=f\"\\n{'-'*10}\\n\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents([data])\n",
    "print(texts[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85defb",
   "metadata": {},
   "source": [
    "### **C. Retrievers**\n",
    "Easy way to combine documents with language models.\n",
    "\n",
    "There are many different types of retrievers, the most widely supported is the `VectoreStoreRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cccbd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader # You will need to install python-magic-bin for this\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# i. Load the environment variables and disable the text splitters verbose output\n",
    "load_dotenv()\n",
    "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
    "\n",
    "# ii. Load text files from the directory\n",
    "loader = DirectoryLoader(\"./Data\", glob=\"**/*.txt\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dab1c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii. Get your splitter ready, setting chunk size to a small number is perfect to retrieve individual cards from an Anki deck\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[f\"\\n{'-'*10}\\n\"], chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "# iv. Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# v. Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vi. Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62372be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize your retriever\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0534bbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001ECD90E6F30>, search_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3846a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to completion: 0.01 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "docs = retriever.invoke(\"What are the causes of back pain?\")\n",
    "print(f\"Time to completion: {round((time.time()-start_time)/60, 2)} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db383cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, CAUSES\n",
      "\n",
      "ACUTE BACK PAIN\n",
      "\n",
      "A. Musculoskeletal:\n",
      "\n",
      "i. Strain or sprain‚Äîmuscu\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN\n",
      "\n",
      "1. The International Association for the Study of Pain (IASP) construed\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, AUS EPIDEMIOLOGY\n",
      "\n",
      "1. In 2014-15: 3.7 million Australians (1 in 6 people\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, INFLAMATORY\n",
      "\n",
      "1. The patient can be young and otherwise in good shape.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out a sample of what was retrieved\n",
    "for doc in docs: print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24193139",
   "metadata": {},
   "source": [
    "### **D. VectorStores**\n",
    "Databases to store vectors. Most popular ones are [Pinecone](https://www.pinecone.io/) & [Weaviate](https://weaviate.io/). More examples on OpenAIs [retriever documentation](https://github.com/openai/chatgpt-retrieval-plugin#choosing-a-vector-database). [Chroma](https://www.trychroma.com/) & [FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/) are easy to work with locally.\n",
    "\n",
    "Conceptually, think of them as tables w/ a column for embeddings (vectors) and a column for metadata.\n",
    "\n",
    "Example\n",
    "\n",
    "| Embedding      | Metadata |\n",
    "| ----------- | ----------- |\n",
    "| [-0.00015641732898075134, -0.003165106289088726, ...]      | {'date' : '1/2/23}       |\n",
    "| [-0.00035465431654651654, 1.4654131651654516546, ...]   | {'date' : '1/3/23}        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45884b9-607b-41a9-b3de-e979d216d535",
   "metadata": {},
   "source": [
    "<br/>**i. Create the vector storage database**\n",
    "<br/>Before you save the vectore store locally you'll need to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c5533ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 3279 documents\n",
      "You have 3279 embeddings\n",
      "Here's a sample of one: [-0.0043275547213852406, 0.008517726324498653, 0.0008955633966252208]...\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import DirectoryLoader # You will need to install python-magic-bin for this\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# i. Load the environment variables and disable the text splitters verbose output\n",
    "load_dotenv()\n",
    "logging.getLogger(\"langchain_text_splitters.base\").setLevel(logging.ERROR)\n",
    "\n",
    "# ii. Load text files from the directory\n",
    "loader = DirectoryLoader(\"./Data\", glob=\"**/*.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# iii. Get your splitter ready, setting chunk size to a small number is perfect to retrieve individual cards from an Anki deck\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators=[f\"\\n{'-'*10}\\n\"], chunk_size=10, chunk_overlap=0)\n",
    "\n",
    "# iv. Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# v. Get embedding engine ready\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# vi. Embedd your texts\n",
    "db = FAISS.from_documents(texts, embeddings)\n",
    "\n",
    "# vii. Inspect your embeddings\n",
    "print (f\"You have {len(texts)} documents\")\n",
    "embedding_list = embeddings.embed_documents([text.page_content for text in texts])\n",
    "print (f\"You have {len(embedding_list)} embeddings\")\n",
    "print (f\"Here's a sample of one: {embedding_list[0][:3]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6e8c42-7f83-42cd-b494-e81760ae8df9",
   "metadata": {},
   "source": [
    "<br/>**ii. Store it locally**\n",
    "<br/>FAISS makes it easy to store embeddings locally. You store your embeddings (‚òùÔ∏è) and make them easily searchable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "088dbf8c-a298-4307-8279-db679953f629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viii. Save the embeddings in a local directory\n",
    "db.save_local(\"./Data/FAISS_embeddings_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711caf82-e167-4da3-b673-86aa074b1260",
   "metadata": {},
   "source": [
    "<br/>**iii. Load the database**\n",
    "<br/>FAISS makes it easy to load and use embeddings quickly. \n",
    "\n",
    "Do note that the de-serialization relies loading a pickle file. Pickle files can be modified to deliver a malicious payload that results in execution of arbitrary code on your machine.You will need to set `allow_dangerous_deserialization` to `True` to enable deserialization. If you do this, make sure that you trust the source of the data. For example, if you are loading a file that you created, and know that no one else has modified the file, then this is safe to do. Do not set this to `True` if you are loading a file from an untrusted source (e.g., some random site on the internet.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6435b340-ef09-4f58-965d-fb1fa2b070f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, CAUSES\n",
      "\n",
      "ACUTE BACK PAIN\n",
      "\n",
      "A. Musculoskeletal:\n",
      "\n",
      "i. Strain or sprain‚Äîmuscu\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN\n",
      "\n",
      "1. The International Association for the Study of Pain (IASP) construed\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, ACTIVITY\n",
      "\n",
      "1. Multiple studies have shown that maintaining activity is a\n",
      "\n",
      "----------\n",
      "\n",
      "PAIN-BACK PAIN, INFLAMATORY\n",
      "\n",
      "1. The patient can be young and otherwise in good shape.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# i. Load the embeddings database\n",
    "load_dotenv()\n",
    "fais_db = FAISS.load_local(\"./Data/FAISS_embeddings_db\", embeddings=OpenAIEmbeddings(), allow_dangerous_deserialization=True)\n",
    "\n",
    "# ii. Initialize your retriever\n",
    "retriever = fais_db.as_retriever()\n",
    "\n",
    "# iii. Query the database\n",
    "docs = retriever.invoke(\"What are the causes of acute back pain?\")\n",
    "\n",
    "# iv. Print out a sample of what was retrieved\n",
    "for doc in docs: print(doc.page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e21a2e2-ee6f-4664-b3ae-e1420f92c061",
   "metadata": {},
   "source": [
    "We can also pass search parameters, such as limiting the number of documents `k` returned by the retriever."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573bc6e9-9fd9-4607-a456-33ee698bab47",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = fais_db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": 0.2})\n",
    "\n",
    "retriever = fais_db.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9b79b",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 6. Memory\n",
    "*Storing chat history*\n",
    "\n",
    "----  \n",
    "Helping LLMs remember information.\n",
    "\n",
    "Memory is a bit of a loose term. It could be as simple as remembering information you've chatted about in the past or more complicated information retrieval.\n",
    "\n",
    "We'll keep it towards the Chat Message use case. This would be used for chat bots.\n",
    "\n",
    "Memory is handeled poorly by LangChain, which is ironic since this is one of its *primary functions!* LangChain instead uses LangGraph persistence to incorporate memory into new LangChain applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43b49da",
   "metadata": {},
   "source": [
    "### A. Chat Message History\n",
    "This helps the chat bot remember your conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893a18c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# i. Instantiate the chat model\n",
    "load_dotenv()\n",
    "chat = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# ii. Create a chat history object\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# iii. Add the AI and user messages\n",
    "history.add_ai_message(\"hi!\")\n",
    "history.add_user_message(\"what is the capital of france?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2949fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b74d5cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "ai_response = chat.invoke(history.messages)\n",
    "print(ai_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "529e168f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content='hi!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='what is the capital of france?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.add_ai_message(ai_response.content)\n",
    "history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d1cb8-ddca-454c-afdc-127ecea0f66a",
   "metadata": {},
   "source": [
    "### B. Automatic history management\n",
    "LangChain also provides a way to build applications that have memory using LangGraph's persistence. You can enable persistence in LangGraph applications by providing a `checkpointer` when compiling the graph. This is unfortunately more convoluted, but the above methods have been deprecated. Read about it [here](https://python.langchain.com/docs/how_to/chatbots_memory/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d8fe8e6-fdea-40ce-b846-35d4c113ed2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# i. Define the model and workflow\n",
    "load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# ii. Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a medical doctor. \"\n",
    "        \"You are answering questions from a medical insurer about a patient you saw in clinic. \"\n",
    "        \"Give detailed answers and interpret and summarize using medical terms/medical jargon where appropriate.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# iii. Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# iv. Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430df929-6a46-43f0-83bd-a0e8bd1de0c8",
   "metadata": {},
   "source": [
    "We'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a75fbba-ef2e-468c-8961-bb1339e46923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a medical doctor providing clinical insights and detailed answers regarding patient care and medical inquiries. My training encompasses a wide range of medical knowledge and terminology, allowing me to communicate effectively about patient conditions, treatments, and relevant clinical data. How can I assist you today regarding the patient's case?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who are you communicating with?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am communicating with a medical insurer regarding a specific patient case. The purpose of this communication is to provide detailed clinical information, including the patient's diagnosis, treatment plan, and any relevant medical history, to support the evaluation of insurance claims and ensure proper coverage for the patient's healthcare needs. If you have specific questions or require information about the patient's clinical status, please let me know, and I will provide the necessary details.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are you communicating about?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am communicating about the clinical details of a patient I have seen in my practice. This includes providing information about the patient's medical history, presenting symptoms, diagnosis, treatment plan, and any relevant diagnostic test results. The goal is to ensure that the medical insurer has a comprehensive understanding of the patient's condition and the rationale for the proposed treatment, thus facilitating appropriate coverage and reimbursement for the medical services rendered. If there are specific aspects of the patient's case you would like me to elaborate on, please let me know.\n"
     ]
    }
   ],
   "source": [
    "human_message = \"Who are you?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"Who are you communicating with?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"What are you communicating about?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "# Pretty print\n",
    "state = app.get_state({\"configurable\": {\"thread_id\": \"1\"}}).values\n",
    "for message in state[\"messages\"]: message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f652965-56f1-4cbe-94b8-95e5cf91360d",
   "metadata": {},
   "source": [
    "### C. Trimming messages\n",
    "LLMs and chat models have limited context windows, and even if you're not directly hitting limits, you may want to limit the amount of distraction the model has to deal with. One solution is trim the history messages before passing them to the model. Let's use an example history with the `app` we declared above.\n",
    "\n",
    "We can see the app remembers the preloaded name.\n",
    "\n",
    "But let's say we have a very small context window, and we want to trim the number of messages passed to the model to only the 2 most recent ones. We can use the built in `trim_messages` util to trim messages based on their token count before they reach our prompt. In this case we'll count each message as 1 \"token\" and keep only the last two messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "516b8dcf-92bf-4ff0-91fe-aa6d428ee533",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import trim_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# i. Define the model\n",
    "load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# ii. Define trimmer & workflow, count each message as 1 \"token\" (token_counter=len) and keep only the last two messages\n",
    "trimmer = trim_messages(strategy=\"last\", max_tokens=2, token_counter=len)\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "\n",
    "# iii. Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    trimmed_messages = trimmer.invoke(state[\"messages\"])\n",
    "    system_prompt = (\n",
    "        \"You are a medical doctor. \"\n",
    "        \"You are answering questions from a medical insurer about a patient you saw in clinic. \"\n",
    "        \"Give detailed answers and interpret and summarize using medical terms/medical jargon where appropriate.\"\n",
    "    )\n",
    "    messages = [SystemMessage(content=system_prompt)] + trimmed_messages\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": response}\n",
    "\n",
    "\n",
    "# iv. Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# v. Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08499f8a-3062-4b84-b6e2-4e250bf60bba",
   "metadata": {},
   "source": [
    "We'll pass the latest input to the conversation here and let LangGraph keep track of the conversation history using the checkpointer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84499dd2-9c5f-47d8-a9ca-8653fb7a829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am a medical professional providing detailed and accurate information regarding patient evaluations, diagnoses, and treatment plans. My expertise allows me to communicate effectively with medical insurers and other healthcare stakeholders about clinical matters, ensuring that all information adheres to medical standards and terminology. How may I assist you with your inquiries today?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Who are you communicating with?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am communicating with a medical insurer regarding a patient I evaluated in clinic. This involves providing detailed clinical information, including the patient's medical history, diagnosis, treatment plan, and any relevant findings that justify the medical necessity of services rendered. My goal is to ensure that the insurer has a comprehensive understanding of the patient's condition and the rationale for the management approach taken. How can I assist you further in this context?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What are you communicating about?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I am communicating about a specific patient encounter in which I assessed the patient's medical condition, formulated a diagnosis, and developed a treatment plan. The communication typically includes:\n",
      "\n",
      "1. **Patient Identification**: Basic demographic information such as age, sex, and relevant medical history.\n",
      "\n",
      "2. **Chief Complaint**: The primary reason the patient presented to the clinic.\n",
      "\n",
      "3. **History of Present Illness (HPI)**: A detailed narrative of the patient's current symptoms, including onset, duration, intensity, and any exacerbating or alleviating factors.\n",
      "\n",
      "4. **Past Medical History**: A summary of any previous medical conditions, surgeries, and treatments that may be relevant to the current visit.\n",
      "\n",
      "5. **Medications**: A list of current medications, including dosages and adherence issues, if any.\n",
      "\n",
      "6. **Physical Examination Findings**: Objective findings noted during the examination that support the diagnosis. This may include vital signs, general appearance, and specific system evaluations.\n",
      "\n",
      "7. **Diagnostic Tests**: Any laboratory tests, imaging studies, or other diagnostic procedures that were performed and their results.\n",
      "\n",
      "8. **Assessment/Diagnosis**: A clear statement of the clinical diagnosis based on the findings and any differential diagnoses considered.\n",
      "\n",
      "9. **Plan of Care**: A detailed outline of the treatment plan, which may include medications, referrals to specialists, follow-up appointments, and lifestyle modifications.\n",
      "\n",
      "10. **Medical Necessity**: Justification for the services rendered, explaining why the chosen interventions are appropriate for the patient's condition and necessary for their health outcomes.\n",
      "\n",
      "This structured communication ensures that the insurer has a clear understanding of the clinical context and supports the rationale for coverage of the services provided. Please let me know if you need specific information about a particular case or component of this communication.\n"
     ]
    }
   ],
   "source": [
    "human_message = \"Who are you?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"Who are you communicating with?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"What are you communicating about?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "# Pretty print\n",
    "state = app.get_state({\"configurable\": {\"thread_id\": \"1\"}}).values\n",
    "for message in state[\"messages\"]: message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b1171-a960-4d64-99eb-c58484fe5906",
   "metadata": {},
   "source": [
    "### D. Summary memory\n",
    "We can use this same pattern in other ways too. For example, we could use an additional LLM call to generate a summary of the conversation before calling our app. Let's recreate our chat history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9096377e-5568-4161-a9cb-2626cca2c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, RemoveMessage\n",
    "\n",
    "# i. Define the model and workflow\n",
    "load_dotenv()\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# ii. Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    system_prompt = (\n",
    "        \"You are a medical doctor. \"\n",
    "        \"You are answering questions from a medical insurer about a patient you saw in clinic. \"\n",
    "        \"Give detailed answers and interpret and summarize using medical terms/medical jargon where appropriate. \"\n",
    "        \"The provided chat history includes a summary of the earlier conversation.\"\n",
    "    )\n",
    "    system_message = SystemMessage(content=system_prompt)\n",
    "    message_history = state[\"messages\"][:-1]  # exclude the most recent user input\n",
    "    # Summarize the messages if the chat history reaches a certain size\n",
    "    if len(message_history) >= 3:\n",
    "        last_human_message = state[\"messages\"][-1]\n",
    "        # Invoke the model to generate conversation summary\n",
    "        summary_prompt = (\n",
    "            \"Distill the above chat messages into a single summary message. \"\n",
    "            \"Include as many specific details as you can.\"\n",
    "        )\n",
    "        summary_message = model.invoke(\n",
    "            message_history + [HumanMessage(content=summary_prompt)]\n",
    "        )\n",
    "\n",
    "        # Delete messages that we no longer want to show up\n",
    "        delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"]]\n",
    "        # Re-add user message\n",
    "        human_message = HumanMessage(content=last_human_message.content)\n",
    "        # Call the model with summary & response\n",
    "        response = model.invoke([system_message, summary_message, human_message])\n",
    "        message_updates = [summary_message, human_message, response] + delete_messages\n",
    "    else:\n",
    "        message_updates = model.invoke([system_message] + state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": message_updates}\n",
    "\n",
    "# iv. Define the node and edge\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.add_edge(START, \"model\")\n",
    "\n",
    "# v. Add simple in-memory checkpointer\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17a05bc0-6c5a-4bcb-8bc9-5945536000ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "In this conversation, you inquired about the number of questions you had asked, and I noted that you had asked one. You then requested a summary of our previous messages, which I provided. Following that, you asked about my identity, and I clarified that I am a medical doctor providing responses related to patient care and clinical assessments for a medical insurer's assistance. I also mentioned that I am communicating about a specific patient evaluation, sharing detailed clinical information, diagnosis, treatment plans, and medical history to aid the insurer in decision-making regarding coverage and care management.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "How many questions, in total, did I ask you before this one?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Before this question, you asked a total of three questions.\n"
     ]
    }
   ],
   "source": [
    "human_message = \"Who are you?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"Who are you communicating with?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"What are you communicating about?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "human_message = \"How many questions, in total, did I ask you before this one?\"\n",
    "app.invoke({\"messages\": [HumanMessage(content=human_message)]}, config={\"configurable\": {\"thread_id\": \"1\"}})\n",
    "\n",
    "# Pretty print\n",
    "state = app.get_state({\"configurable\": {\"thread_id\": \"1\"}}).values\n",
    "for message in state[\"messages\"]: message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29fc79c",
   "metadata": {},
   "source": [
    "#\n",
    "<img src=\"Images/atom.png\" alt=\"Atom\" style=\"width:60px\" align=\"left\" vertical-align=\"middle\">\n",
    "\n",
    "## 7. Chains ‚õìÔ∏è‚õìÔ∏è‚õìÔ∏è\n",
    "*Storing chat history*\n",
    "\n",
    "----  \n",
    "Combining different LLM calls and action automatically.\n",
    "\n",
    "Ex: Summary #1, Summary #2, Summary #3 > Final Summary.\n",
    "\n",
    "LangChain has its own language, LangChain Expression Language (LCEL), the basic expression of which is:\n",
    "> `chain = prompt | model`\n",
    "> \n",
    "> `result = chain.invoke({key:value})`\n",
    "\n",
    "Where the pipe operator chains the prompt to the model.\n",
    "\n",
    "There are many chain possibilities: *extended (linear), parallel, branching.* \n",
    "\n",
    "We'll cover one of them, which is the extended (linear) chain and another called the summarization chain, whcih is simply a more elaborate extended chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba415",
   "metadata": {},
   "source": [
    "### A. Simple Sequential Chains (Extended chains)\n",
    "\n",
    "Easy chains where you can use the output of an LLM as an input into another. Good for breaking up tasks (and keeping your LLM focused)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79fc0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# i. Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# ii. Create a ChatOpenAI model\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43d4494a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iii. Define prompt templates (no need for separate Runnable chains)\n",
    "template = \"\"\"Your job is to come up with a classic dish from the area that the users suggests.\n",
    "USER LOCATION:\n",
    "{user_location}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template1 = PromptTemplate(input_variables=[\"user_location\"], template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18259e76-2d97-4d60-95c3-79a7da80019c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One classic dish from Rome is \"Cacio e Pepe.\" This traditional Roman pasta dish is known for its simplicity and rich, savory flavors. It consists of just three main ingredients: Pecorino Romano cheese, black pepper, and pasta, typically spaghetti or tonnarelli. The dish highlights the quality and flavor of the ingredients, with the creamy cheese and pepper creating a luscious sauce that coats the pasta. Cacio e Pepe is a quintessential Roman comfort food and a must-try for anyone visiting the city.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iv. Create the combined chain using LangChain Expression Language (LCEL)\n",
    "chain = prompt_template1 | model | StrOutputParser()\n",
    "chain.invoke({'user_location':'Rome'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e87bf7-5fc2-4654-aaf8-6352c01ca1ba",
   "metadata": {},
   "source": [
    "<br/>We can create extended chains as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6c8e00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Cacio e Pepe Recipe**\\n\\n**Ingredients:**\\n- 12 oz spaghetti or tonnarelli\\n- 1 cup Pecorino Romano cheese, finely grated\\n- 2 tsp freshly ground black pepper\\n- Salt, for pasta water\\n\\n**Instructions:**\\n\\n1. **Boil the Pasta:** Bring a large pot of salted water to a boil. Add the spaghetti and cook until al dente, according to package instructions. Reserve about 1 cup of the pasta water, then drain the pasta.\\n\\n2. **Prepare the Sauce:** In a large skillet or pan over medium heat, toast the freshly ground black pepper for about 1-2 minutes until fragrant.\\n\\n3. **Combine:** Add about 1/2 cup of the reserved pasta water to the skillet with the pepper. Add the drained pasta and toss to coat.\\n\\n4. **Add Cheese:** Remove the skillet from the heat and gradually sprinkle in the Pecorino Romano cheese, tossing constantly. Add more reserved pasta water, a little at a time, if needed, to create a creamy sauce that coats the pasta well.\\n\\n5. **Serve:** Serve immediately, with extra Pecorino Romano and black pepper on top if desired.\\n\\nEnjoy your homemade taste of Rome!'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define another template\n",
    "template = \"\"\"Given a meal, give a short and simple recipe on how to make that dish at home.\n",
    "MEAL:\n",
    "{user_meal}\n",
    "\n",
    "YOUR RESPONSE:\n",
    "\"\"\"\n",
    "prompt_template2 = PromptTemplate(input_variables=[\"user_meal\"], template=template)\n",
    "\n",
    "# Chain everything together!\n",
    "chain = prompt_template1 | model | StrOutputParser() | prompt_template2 | model | StrOutputParser()\n",
    "chain.invoke({'user_location':'Rome'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f8557c-c3b9-45e6-b446-faa1e146d37a",
   "metadata": {},
   "source": [
    "<br/>You can even define additional processing steps using `RunnableLambda` and chain these processes with the extended chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdb6d15e-8661-450e-b607-f9c95c5c5e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**CACIO E PEPE RECIPE**\\n\\n**INGREDIENTS:**\\n- 400G SPAGHETTI OR TONNARELLI\\n- 200G PECORINO ROMANO CHEESE, FINELY GRATED\\n- 2 TEASPOONS FRESHLY GROUND BLACK PEPPER\\n- SALT (FOR PASTA WATER)\\n\\n**INSTRUCTIONS:**\\n\\n1. **COOK THE PASTA**: BRING A LARGE POT OF SALTED WATER TO A BOIL. ADD THE PASTA AND COOK UNTIL AL DENTE ACCORDING TO PACKAGE INSTRUCTIONS. RESERVE ABOUT 1 CUP OF THE PASTA COOKING WATER, THEN DRAIN THE PASTA.\\n\\n2. **TOAST THE PEPPER**: WHILE THE PASTA COOKS, HEAT A LARGE, DEEP SKILLET OVER MEDIUM HEAT. ADD THE FRESHLY GROUND BLACK PEPPER AND TOAST IT FOR ABOUT 1 MINUTE UNTIL FRAGRANT.\\n\\n3. **CREATE THE SAUCE**: LOWER THE HEAT TO MEDIUM-LOW. ADD A SMALL AMOUNT OF THE RESERVED PASTA WATER TO THE SKILLET WITH THE TOASTED PEPPER. GRADUALLY ADD THE GRATED PECORINO ROMANO CHEESE, STIRRING CONSTANTLY TO CREATE A CREAMY SAUCE. ADD MORE PASTA WATER AS NEEDED TO REACH A SMOOTH CONSISTENCY.\\n\\n4. **COMBINE**: ADD THE DRAINED PASTA TO THE SKILLET, TOSSING IT WITH THE CHEESE AND PEPPER SAUCE UNTIL WELL COATED. IF THE SAUCE IS TOO THICK, ADD MORE PASTA WATER, A LITTLE AT A TIME, TO ACHIEVE THE DESIRED CREAMINESS.\\n\\n5. **SERVE**: DIVIDE THE PASTA AMONG SERVING PLATES AND GARNISH WITH ADDITIONAL PECORINO ROMANO AND A SPRINKLE OF BLACK PEPPER IF DESIRED.\\n\\nENJOY YOUR HOMEMADE CACIO E PEPE, A TASTE OF ROME IN YOUR OWN KITCHEN!'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Define additional processing steps using RunnableLambda\n",
    "uppercase_output = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Word count: {len(x.split())}\\n{x}\")\n",
    "\n",
    "# Chain everything together!\n",
    "chain = prompt_template1 | model | StrOutputParser() | prompt_template2 | model | StrOutputParser() | uppercase_output\n",
    "chain.invoke({'user_location':'Rome'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6191bf5",
   "metadata": {},
   "source": [
    "### 2. Summarization Chain\n",
    "\n",
    "Easily run through long numerous documents and get a summary. Check out [this video](https://www.youtube.com/watch?v=f9_BWhCI4Zo) for other chain types besides map-reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6f218c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"January 2017Because biographies of famous scientists tend to \n",
      "edit out their mistakes, we underestimate the \n",
      "degree of risk they were willing to take.\n",
      "And because anything a famous scientist did that\n",
      "wasn't a mistake has probably now become the\n",
      "conventional wisdom, those choices don't\n",
      "seem risky either.Biographies of Newton, for example, understandably focus\n",
      "more on physics than alchemy or theology.\n",
      "The impression we get is that his unerring judgment\n",
      "led him straight to truths no one else had noticed.\n",
      "How to explain all the time he spent on alchemy\n",
      "and theology?  Well, smart people are often kind of\n",
      "crazy.But maybe there is a simpler explanation. Maybe\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the smartness and the craziness were not as separate\n",
      "as we think. Physics seems to us a promising thing\n",
      "to work on, and alchemy and theology obvious wastes\n",
      "of time. But that's because we know how things\n",
      "turned out. In Newton's day the three problems \n",
      "seemed roughly equally promising. No one knew yet\n",
      "what the payoff would be for inventing what we\n",
      "now call physics; if they had, more people would \n",
      "have been working on it. And alchemy and theology\n",
      "were still then in the category Marc Andreessen would \n",
      "describe as \"huge, if true.\"Newton made three bets. One of them worked. But \n",
      "they were all risky.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\" Biographies of famous scientists often edit out their mistakes, giving readers the wrong impression that they never faced any risks to achieve successful results. An example of this is Newton, whose smartness is assumed to have led straight him to truths without any detours into alchemy or theology - despite the fact that he spent a lot of time on both fields. Maybe the simpler explanation is that he was willing to take risks, even if it means potentially making mistakes.\n",
      "\n",
      " In the 17th century, Newton took a risk and made three bets, one of which turned out to be a successful invention of what we now call physics. The other two bets were on less popular subjects of the time such as alchemy and theology. People did not know then what the payoff would be, but the bets still seemed relatively promising.\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Biographies tend to omit famous scientists' mistakes from their stories, but Newton was willing to take risks and explore multiple fields to make his discoveries. He placed three risky bets, one of which resulted in the creation of physics as we know it today.\""
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = TextLoader('data/PaulGrahamEssays/disc.txt')\n",
    "documents = loader.load()\n",
    "\n",
    "# Get your splitter ready\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=50)\n",
    "\n",
    "# Split your docs into texts\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# There is a lot of complexity hidden in this one line. I encourage you to check out the video above for more detail\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f6193c",
   "metadata": {},
   "source": [
    "## Agents ü§ñü§ñ\n",
    "\n",
    "Official LangChain Documentation describes agents perfectly (emphasis mine):\n",
    "> Some applications will require not just a predetermined chain of calls to LLMs/other tools, but potentially an **unknown chain** that depends on the user's input. In these types of chains, there is a ‚Äúagent‚Äù which has access to a suite of tools. Depending on the user input, the agent can then **decide which, if any, of these tools to call**.\n",
    "\n",
    "\n",
    "Basically you use the LLM not just for text output, but also for decision making. The coolness and power of this functionality can't be overstated enough.\n",
    "\n",
    "Sam Altman emphasizes that the LLMs are good '[reasoning engine](https://www.youtube.com/watch?v=L_Guz73e6fw&t=867s)'. Agent take advantage of this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce05d51",
   "metadata": {},
   "source": [
    "### Agents\n",
    "\n",
    "The language model that drives decision making.\n",
    "\n",
    "More specifically, an agent takes in an input and returns a response corresponding to an action to take along with an action input. You can see different types of agents (which are better for different use cases) [here](https://python.langchain.com/en/latest/modules/agents/agents/agent_types.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f696b65c",
   "metadata": {},
   "source": [
    "### Tools\n",
    "\n",
    "A 'capability' of an agent. This is an abstraction on top of a function that makes it easy for LLMs (and agents) to interact with it. Ex: Google search.\n",
    "\n",
    "This area shares commonalities with [OpenAI plugins](https://platform.openai.com/docs/plugins/introduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11f8231",
   "metadata": {},
   "source": [
    "### Toolkit\n",
    "\n",
    "Groups of tools that your agent can select from\n",
    "\n",
    "Let's bring them all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67d5d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "llm = OpenAI(temperature=0, openai_api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0ddcdbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "serpapi_api_key=os.getenv(\"SERP_API_KEY\", \"YourAPIKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "44fad67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = load_tools([\"serpapi\"], llm=llm, serpapi_api_key=serpapi_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f544a74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(toolkit, llm, agent=\"zero-shot-react-description\", verbose=True, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c4882754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I should try to find out what band Natalie Bergman is a part of.\n",
      "Action: Search\n",
      "Action Input: \"Natalie Bergman band\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, was released on Third Man Records on May 7, 2021. She is based in Los Angeles.', 'Natalie Bergman type: American singer-songwriter.', 'Natalie Bergman main_tab_text: Overview.', 'Natalie Bergman kgmid: /m/0qgx4kh.', 'Natalie Bergman genre: Folk.', 'Natalie Bergman parents: Susan Bergman, Judson Bergman.', 'Natalie Bergman born: 1988 or 1989 (age 34‚Äì35).', 'Natalie Bergman is an American singer-songwriter. She is one half of the duo Wild Belle, along with her brother Elliot Bergman. Her debut solo album, Mercy, ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I should search for the first album of Wild Belle\n",
      "Action: Search\n",
      "Action Input: \"Wild Belle first album\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mIsles\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Isles is the first album of the band that Natalie Bergman is a part of.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "response = agent({\"input\":\"what was the first album of the\" \n",
    "                    \"band that Natalie Bergman is a part of?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9c30d2",
   "metadata": {},
   "source": [
    "![Wild Belle](data/WildBelle1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f4b368",
   "metadata": {},
   "source": [
    "üéµEnjoyüéµ\n",
    "https://open.spotify.com/track/1eREJIBdqeCcqNCB1pbz7w?si=c014293b63c7478c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
